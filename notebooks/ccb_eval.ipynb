{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "small = ['small1.json', 'small2.json']\n",
    "large = ['01_0.json']\n",
    "\n",
    "files = small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Total'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['_cache\\\\cache-p\\\\9994b6a3ba3c46af807259c8b5d13f70',\n",
       " '_cache\\\\cache-p\\\\0bc0717ead83e333f080721b1355126e']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from VwPipeline import Loggers, Handlers\n",
    "from VwPipeline.VwCache import VwCache\n",
    "from VwPipeline.Vw import Vw\n",
    "from VwPipeline.VwOpts import dimension, product\n",
    "import pandas as pd\n",
    "\n",
    "#your vw path\n",
    "vw_path = r'C:\\vw\\vw.exe'\n",
    "\n",
    "cache = VwCache(r'_cache')\n",
    "vw = Vw(\n",
    "    vw_path,\n",
    "    cache,\n",
    "    handlers=[Handlers.WidgetHandler()],\n",
    "    )\n",
    "\n",
    "opts = pd.DataFrame(product(\n",
    "    dimension('#base', ['--ccb_explore_adf --dsjson --compressed --synthcover --power_t 0  -P 1 --preserve_performance_counters --save_resume']),\n",
    "))\n",
    "preds = vw.train(files, opts, ['-p'])\n",
    "prediction_files = preds.iloc[0]['!Outputs']['-p']\n",
    "prediction_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from itertools import chain\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "class FileSizeHasher:\n",
    "    extension = 'size'\n",
    "    \n",
    "    def evaluate(self, path):\n",
    "        return Path(path).stat().st_size\n",
    "\n",
    "class FilesPipeline:\n",
    "    hasher = FileSizeHasher()\n",
    "    \n",
    "    def _load_hash(self, path):\n",
    "        hash_path = f'{path}.{self.hasher.extension}'\n",
    "        if not Path(hash_path).exists():\n",
    "            return None\n",
    "        try:\n",
    "            hash_value = int(open(hash_path, 'r').read())\n",
    "        except:\n",
    "            hash_value = None\n",
    "        return hash_value\n",
    "    \n",
    "    def _is_in_sync(self, inp, output):\n",
    "        input_hash = self.hasher.evaluate(inp)\n",
    "        output_hash = self._load_hash(output)\n",
    "        return input_hash and output_hash and input_hash == output_hash\n",
    "\n",
    "    def _sync(self, inp, output):\n",
    "        with open(f'{output}.{self.hasher.extension}','w') as f:\n",
    "            f.write(str(self.hasher.evaluate(inp)))\n",
    "    \n",
    "    def __init__(self, hasher = None):\n",
    "        self.hasher = hasher if hasher is not None else self.hasher\n",
    "    \n",
    "    def process(self, \n",
    "        files,\n",
    "        processor,\n",
    "        path_gen=None,\n",
    "        process=False):\n",
    "        path_gen = path_gen or (lambda f: f'{f}.{processor.__name__}') \n",
    "        result = []\n",
    "        for path_in in files:\n",
    "            print(f'Processing {path_in}...')\n",
    "            path_out = path_gen(path_in)\n",
    "            Path(path_out).parent.mkdir(parents=True, exist_ok=True)\n",
    "            if process or not self._is_in_sync(path_in, path_out):\n",
    "                with open(path_out, 'w') as fout:\n",
    "                    with open(path_in) as fin:\n",
    "                        fout.writelines(processor(fin))\n",
    "                self._sync(path_in, path_out)\n",
    "            if Path(path_out).exists():\n",
    "                result.append(path_out)\n",
    "        return result\n",
    "\n",
    "def files_2_csvs(\n",
    "    files,\n",
    "    processor,\n",
    "    path_gen=None,\n",
    "    process=False):\n",
    "    result = []\n",
    "    for f in files:\n",
    "        print(f'Processing {f}...')\n",
    "        output = path_gen(f)\n",
    "        Path(output).parent.mkdir(parents=True, exist_ok=True)\n",
    "        if process or not _is_in_sync(f, output):\n",
    "            df = pd.DataFrame(processor(open(f)))\n",
    "            if len(df) > 0:\n",
    "                df.to_csv(output, index=False)\n",
    "            _sync(f, output)\n",
    "        if Path(output).exists():\n",
    "            result.append(output)\n",
    "    return result\n",
    "\n",
    "def csvs_2_rows(files, processors=[]):\n",
    "    if not processors:\n",
    "        processors = [lambda d: d]\n",
    "    for kv in chain.from_iterable(map(lambda f: pd.read_csv(f).iterrows(), files)):\n",
    "        yield ChainMap(*[p(kv[1]) for p in processors])\n",
    "\n",
    "def ndjsons_2_rows(files, processors=[]):\n",
    "    if not processors:\n",
    "        processors = [lambda d: d]\n",
    "    for o in map(lambda l: json.loads(l), chain.from_iterable(map(lambda f: open(f), files))):\n",
    "        yield o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "import uuid\n",
    "import pandas as pd\n",
    "from collections import ChainMap\n",
    "\n",
    "def is_ccb_decision(line):\n",
    "        return line.startswith('{\"Timestamp\"')\n",
    "    \n",
    "class UniformSampler:\n",
    "    counter = 0\n",
    "    \n",
    "    def __init__(self, fraction):\n",
    "        self.period = int(1/fraction)\n",
    "        \n",
    "    def do(self, line):\n",
    "        self.counter = (self.counter + 1) % self.period\n",
    "        return self.counter == 0\n",
    "\n",
    "def sample(fraction):\n",
    "    import random\n",
    "    return random.random() < fraction\n",
    "\n",
    "class DsJsonCcb:   \n",
    "    default_top_processor = lambda o: {\n",
    "        'Timestamp': o['Timestamp'],\n",
    "        '_skipLearn': False if '_skipLearn' not in o else o['_skipLearn'],\n",
    "        'pdrop': 0.0 if 'pdrop' not in o else o['pdrop']}\n",
    "    \n",
    "    default_slot_processor = lambda o: {\n",
    "        '_inc': o['_inc'] if '_inc' in o else []}\n",
    "    \n",
    "    default_outcome_processor = lambda o: {\n",
    "        '_label_cost': o['_label_cost'],\n",
    "        '_id': o['_id'],\n",
    "        '_a': o['_a'],\n",
    "        '_p': o['_p']} \n",
    "    \n",
    "    processors = {\n",
    "        '/': [default_top_processor],\n",
    "        'c': [],\n",
    "        '_multi': [],\n",
    "        '_slots': [default_slot_processor],\n",
    "        '_outcomes': [default_outcome_processor]\n",
    "    }\n",
    "\n",
    "    filters = [is_ccb_decision]\n",
    "\n",
    "    def __init__(self, processors = None, filters = None):\n",
    "        self.processors = processors if processors is not None else self.processors\n",
    "        self.filters = filters if filters is not None else self.filters\n",
    "\n",
    "    def process_decision(self, line):\n",
    "        parsed = json.loads(line)\n",
    "        top = dict(ChainMap(*[p(parsed) for p in self.processors['/']]))\n",
    "        shared = dict(ChainMap(*[p(parsed['c']) for p in self.processors['c']]))\n",
    "\n",
    "        actions = [None] * len(parsed['c']['_multi'])\n",
    "        for i, o in enumerate(parsed['c']['_multi']):\n",
    "            actions[i] = dict(ChainMap(*[p(o) for p in self.processors['_multi']]))\n",
    "\n",
    "        slots = [None] * len(parsed['c']['_slots'])\n",
    "        for i, o in enumerate(parsed['c']['_slots']):\n",
    "            slots[i] = dict(ChainMap(*[p(o) for p in self.processors['_slots']]))\n",
    "        \n",
    "        outcomes = [None] * len(parsed['_outcomes'])\n",
    "        for i, o in enumerate(parsed['_outcomes']):\n",
    "            outcomes[i] = dict(ChainMap(*[p(o) for p in self.processors['_outcomes']]))\n",
    "        \n",
    "        result = dict(ChainMap(top, {'c': dict(ChainMap(shared, {'_multi': actions, '_slots': slots})), '_outcomes': outcomes}))\n",
    "        return json.dumps(result)\n",
    "        \n",
    "    def ndjson_2_slots(self, line):\n",
    "        parsed = json.loads(line)\n",
    "        session = {'Session': str(uuid.uuid4()),\n",
    "                    'T': pd.to_datetime(parsed['shared']['T']),\n",
    "                    'NumActions': len(parsed['_multi']),\n",
    "                    'NumSlots': len(parsed['_slots']),\n",
    "                    'SkipLearn': False if '_skipLearn' not in parsed['shared'] else parsed['shared']['_skipLearn'],\n",
    "                    'Pdrop': 0.0 if 'pdrop' not in parsed['shared'] else parsed['shared']['pdrop']}\n",
    "        session_custom = [p(parsed['shared']) for p in self.context_processors]\n",
    "\n",
    "        slots = [None] * len(parsed['slots'])\n",
    "        for i, o in enumerate(parsed['slots']):\n",
    "            slots[i] = ChainMap({'SlotIdx': i,\n",
    "                    'Reward': o['Reward'],\n",
    "                    'Id': o['Id'],\n",
    "                    'ActionsPerSlot': len(o['_a']),\n",
    "                    'Chosen': o['Chosen'],\n",
    "                    'Prob': o['P']},\n",
    "                    *[p(o) for p in self.slot_processors])\n",
    "        \n",
    "        return map(lambda s: ChainMap(session, *session_custom, s), slots)\n",
    "    \n",
    "    def process(self, lines):\n",
    "        for f in self.filters:\n",
    "            lines = filter(lambda l: f(l), lines)\n",
    "        if len(self.processors) > 1:\n",
    "            return map(lambda l: f'{self.process_decision(l)}\\n', lines) \n",
    "        return lines\n",
    "    \n",
    "\n",
    "class VwPredictionsCcb:\n",
    "    @staticmethod\n",
    "    def line_2_slot(line):\n",
    "        return {p.split(':')[0] : float(p.split(':')[1])  for p in line.split(',')}\n",
    "\n",
    "    @staticmethod\n",
    "    def lines_2_slots(lines):\n",
    "        return map(VwPredictionsCcb.line_2_slot, filter(lambda l : not l.isspace(), lines))\n",
    "\n",
    "    @staticmethod\n",
    "    def files_2_slots(files):\n",
    "        return itertools.chain.from_iterable(map(lambda f: VwPredictionsCcb.lines_2_slots(open(f)), files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestamp(row):\n",
    "    return {'T': row['Timestamp']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing small1.json...\n",
      "Processing small2.json...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['processed\\\\small1.json', 'processed\\\\small2.json']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = DsJsonCcb()\n",
    "pipeline = FilesPipeline()\n",
    "\n",
    "result = pipeline.process(small, parser.process, path_gen=lambda p: fr'processed\\{p}', process=True)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing processed\\small1.json...\n",
      "Processing processed\\small2.json...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['processed\\\\small1.json.process', 'processed\\\\small2.json.process']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = DsJsonCcb(filters=[lambda l: True])\n",
    "result = pipeline.process(result, parser.process, process=True)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing small1.json...\n",
      "Processing small2.json...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['small1.json.process', 'small2.json.process']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler = UniformSampler(0.5)\n",
    "\n",
    "parser = DsJsonCcb(filters=[lambda l: sampler.do(l)])\n",
    "result = pipeline.process(small, parser.process, process=True)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(filter(lambda s: s['SkipLearn']==False,\n",
    "    csvs_2_rows(slot_files, processors = [\n",
    "        timestamp\n",
    "    ])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = small\n",
    "\n",
    "parser = DsJsonCcb()\n",
    "slot_files= files_2_csvs(files, parser.lines_2_slots, path_gen=lambda p: fr'processed\\{p}.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slots = filter(lambda s: s['SkipLearn']==False, csvs_2_rows(slot_files))\n",
    "preds = VwPredictionsCcb.files_2_slots(prediction_files)\n",
    "\n",
    "ds = map(lambda kv: ChainMap(kv[0], kv[1]), zip(slots, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(VwPredictionsCcb.files_2_slots(prediction_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(map(lambda kv: ChainMap(kv[0], kv[1]), zip(slots, preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = DsJsonCcb()\n",
    "list(parser.lines_2_slots(open(small[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_result = parser.lines_2_slots(open(large[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(large_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(DsJsonCcb.files_2_slots(inputs,context_processors = [cp]), VwPredictionsCcb.lines_2_slots(open(prediction_file))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(DsJsonCcb.files_2_slots(inputs,context_processors = [cp]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import uuid\n",
    "import itertools\n",
    "import pytz\n",
    "\n",
    "class DsJson:\n",
    "    @staticmethod\n",
    "    def is_ccb_event(line):\n",
    "        try:\n",
    "            o = json.loads(line)\n",
    "        except:\n",
    "            return False\n",
    "        return line.startswith('{\"Timestamp\"')\n",
    "\n",
    "    @staticmethod\n",
    "    def is_cb_event(line):\n",
    "        return line.startswith('{\"_label_cost\"')\n",
    "\n",
    "    @staticmethod\n",
    "    def is_dangling_reward(line):\n",
    "        return line.startswith('{\"RewardValue')\n",
    "\n",
    "    @staticmethod\n",
    "    def get_timestamp(line):\n",
    "        obj = NaiveJson(line)\n",
    "        if line.startswith('{\"RewardValue'):\n",
    "            return pd.to_datetime(obj.get_string(\"EnqueuedTimeUtc\"))\n",
    "        return pd.to_datetime(obj.get_string(\"Timestamp\"))\n",
    "\n",
    "    @staticmethod\n",
    "    def context(line):\n",
    "        parsed = json.loads(line)\n",
    "        return json.dumps(parsed['c']) + '\\n'\n",
    "\n",
    "    @staticmethod\n",
    "    def dangling_reward(line):\n",
    "        parsed = json.loads(line)\n",
    "        return {'Timestamp': pd.to_datetime(parsed['EnqueuedTimeUtc']), 'EventId': parsed['EventId'], 'Reward': parsed['RewardValue']}\n",
    "\n",
    "    @staticmethod\n",
    "    def analyze_observations(obj):\n",
    "        rewards = 0\n",
    "        activations =0\n",
    "        for o in obj:\n",
    "            if 'ActionTaken' in o and o['ActionTaken']==True:\n",
    "                activations = activations + 1\n",
    "            else:\n",
    "                rewards = rewards + 1\n",
    "\n",
    "        return rewards, activations\n",
    "\n",
    "    def get_title_from_obj(action):\n",
    "        c = action['c']\n",
    "        if 'Title' in c:\n",
    "            return c['Title']\n",
    "        elif 'ProductTitle' in c:\n",
    "            return c['ProductTitle']\n",
    "        return None\n",
    "\n",
    "    @staticmethod\n",
    "    def ccb_event(line):\n",
    "        parsed = json.loads(line)\n",
    "        session = {'Session': str(uuid.uuid4()),\n",
    "                 'Timestamp': pd.to_datetime(parsed['Timestamp']),\n",
    "                 'NumActions': len(parsed['c']['_multi']),\n",
    "                 'NumSlots': len(parsed['c']['_slots']),\n",
    "                 'VWState': parsed['VWState']['m'],\n",
    "                 'SkipLearn': False if '_skipLearn' not in parsed else parsed['_skipLearn'],\n",
    "                 'StringLen': len(line),\n",
    "                 'Pdrop': 0.0 if 'pdrop' not in parsed else parsed['pdrop']}\n",
    "\n",
    "        multi = [None] * len(parsed['c']['_multi'])\n",
    "        for i, o in enumerate(parsed['c']['_multi']):\n",
    "            multi[i] = {'Id': o['c']['Id'],\n",
    "                        'Len': len(json.dumps(o))}\n",
    "\n",
    "        slots = [None] * len(parsed['_outcomes'])\n",
    "        for i, o in enumerate(parsed['_outcomes']):\n",
    "            r, a = DsJson.analyze_observations(o['_o'])\n",
    "            slots[i] = {'SlotIdx': i,\n",
    "                    'Cost': o['_label_cost'],\n",
    "                    'EventId': o['_id'],\n",
    "                    'ActionsPerSlot': len(o['_a']),\n",
    "                    'Chosen': o['_a'][0],\n",
    "                    'Prob': o['_p'][0],\n",
    "                    'Rewards': r,\n",
    "                    'Activations': a,\n",
    "                    'Product': multi[o['_a'][0]]['Id'],\n",
    "                    'ChosenActionLen': multi[o['_a'][0]]['Len']}\n",
    "        \n",
    "        return [dict(session, **m) for m in multi] \n",
    "\n",
    "    @staticmethod\n",
    "    def ccb_2_cb(session, slots, multi):\n",
    "        return [dict(session, **s) for s in slots]\n",
    "\n",
    "    @staticmethod\n",
    "    def ccb_as_cb_to_stats(df):\n",
    "        result = df\n",
    "        result['TimestampFloor'] = result.index.floor('1min')\n",
    "        result['TimestampFloor'] = result['TimestampFloor'].dt.tz_localize(None)\n",
    "        result['Observations'] = result['HasObservation'].astype(int).div(1 - result['Pdrop'])\n",
    "        result['Rewards'] = -result['Cost'].div(1 - result['Pdrop'])\n",
    "        result['Events'] = 1\n",
    "        result['EventsLogged'] = result['Events']\n",
    "        result['Events'] = result['Events'].div(1 - result['Pdrop'])\n",
    "        result['RewardsSlot1'] = result['Rewards'].mul((result['SlotIdx']==0).astype(int))\n",
    "        result['EventsSlot1'] = result['Events'].mul((result['SlotIdx']==0).astype(int))\n",
    "        result['RewardsIps1'] = result['Rewards'].mul((result['SlotIdx']==result['Chosen']).astype(int)).div(result['Prob'])\n",
    "        result['EventsIps1'] = result['Events'].mul((result['SlotIdx']==result['Chosen']).astype(int)).div(result['Prob'])\n",
    "        result['RewardsIps1Slot1'] = result['RewardsIps1'].mul((result['SlotIdx']==0).astype(int))\n",
    "        result['EventsIps1Slot1'] = result['EventsIps1'].mul((result['SlotIdx']==0).astype(int))\n",
    "        result['RewardsIpsR'] = result['Rewards'].mul(result['ActionsPerSlot']).div(result['Prob'])\n",
    "        result['EventsIpsR'] = result['Events'].mul(result['ActionsPerSlot']).div(result['Prob'])\n",
    "        result['RewardsIpsRSlot1'] = result['RewardsIpsR'].mul((result['SlotIdx']==0).astype(int))\n",
    "        result['EventsIpsRSlot1'] = result['EventsIpsR'].mul((result['SlotIdx']==0).astype(int))\n",
    "\n",
    "        return result[['TimestampFloor', 'Observations', 'Rewards', 'Events', 'RewardsSlot1', 'EventsSlot1', 'RewardsIps1', 'EventsIps1', 'RewardsIps1Slot1', 'EventsIps1Slot1', 'RewardsIpsR', 'EventsIpsR', 'RewardsIpsRSlot1', 'EventsIpsRSlot1', 'EventsLogged']].reset_index().drop('Timestamp', axis=1).rename(columns = {'TimestampFloor': 'Timestamp'}).groupby('Timestamp').sum()\n",
    "\n",
    "    @staticmethod\n",
    "    def get_title_from_obj(action):\n",
    "        c = action['c']\n",
    "        if 'Title' in c:\n",
    "            return c['Title']\n",
    "        elif 'ProductTitle' in c:\n",
    "            return c['ProductTitle']\n",
    "        return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def ccb_action(line):\n",
    "        parsed = json.loads(line)\n",
    "        session = {'Session': parsed['_outcomes'][0]['_id'], 'Timestamp': pd.to_datetime(parsed['Timestamp'])}\n",
    "        multi = [None] * len(parsed['c']['_multi'])\n",
    "        for i, o in enumerate(parsed['c']['_multi']):\n",
    "            multi[i] = {'Id': o['c']['Id'],\n",
    "                        'Index': i,\n",
    "                        'ChannelId': o['c']['Id'],\n",
    "                        'Title': DsJson.get_title_from_obj(o),\n",
    "                        'SlotIdx': -1,\n",
    "                        'Cost': 0,\n",
    "                        'Prob': 0,\n",
    "                        'ActionLen': len(str(o)),\n",
    "                        'CLen': len(o['c']),\n",
    "                        'DLen': len(o['d']),\n",
    "                        'ELen': len(o['e']),\n",
    "                        'HLen': len(o['h']),                        \n",
    "       #                 'plc0': o['c']['plc0'],\n",
    "       #                 'plc1': o['c']['plc1'],\n",
    "       #                 'plc2': o['c']['plc2'],\n",
    "       #                 'plc3': o['c']['plc3'],\n",
    "       #                 'plc4': o['c']['plc4'],\n",
    "                       }\n",
    "      #      for key in o['c']:\n",
    "      #          multi[i][f'c/{key}'] = o['c'][key]\n",
    "        for i, o in enumerate(parsed['_outcomes']):\n",
    "            multi[o['_a'][0]]['SlotIdx'] = i\n",
    "            multi[o['_a'][0]]['Cost'] = o['_label_cost']\n",
    "            multi[o['_a'][0]]['Prob'] = o['_p'][0]\n",
    "        return [dict(session, **m) for m in multi]      \n",
    "\n",
    "    @staticmethod\n",
    "    def dangling_reward_lines(lines):\n",
    "        return filter(lambda l: DsJson.is_dangling_reward(l), lines)\n",
    "\n",
    "    @staticmethod\n",
    "    def ccb_decision_lines(lines):\n",
    "        return filter(lambda l: DsJson.is_ccb_event(l), lines)\n",
    "    \n",
    "    @staticmethod\n",
    "    def dangling_rewards(lines):\n",
    "        df = pd.DataFrame(\n",
    "            map(lambda l: DsJson.dangling_reward(l), DsJson.dangling_reward_lines(lines)))\n",
    "        return df.set_index('Timestamp') if len(df) > 0 else df\n",
    "\n",
    "    @staticmethod\n",
    "    def ccb_events(lines):\n",
    "        events = map(lambda l: DsJson.ccb_2_cb(*DsJson.ccb_event(l)), DsJson.ccb_decision_lines(lines))\n",
    "        df = pd.DataFrame(itertools.chain(*events))\n",
    "        return df#.set_index('Timestamp')\n",
    "\n",
    "    @staticmethod\n",
    "    def ccb_stats(lines):\n",
    "        events = map(lambda l: DsJson.ccb_2_cb(*DsJson.ccb_event(l)), DsJson.ccb_decision_lines(lines))\n",
    "        df = pd.DataFrame(itertools.chain(*events))\n",
    "        return DsJson.ccb_as_cb_to_stats(df.set_index('Timestamp'))\n",
    "\n",
    "    @staticmethod\n",
    "    def ccb_actions(lines):\n",
    "        actions = map(lambda l: DsJson.ccb_action(l), DsJson.ccb_decision_lines(lines))\n",
    "        df = pd.DataFrame(itertools.chain(*actions))\n",
    "        return df.set_index('Timestamp')\n",
    "\n",
    "    @staticmethod\n",
    "    def contexts(lines):\n",
    "        return map(lambda e: DsJson.context(e),\n",
    "            filter(lambda l: DsJson.is_ccb_event(l), lines))\n",
    "    \n",
    "    @staticmethod\n",
    "    def first_timestamp(lines):\n",
    "        line = next(lines)\n",
    "        return DsJson.get_timestamp(line)\n",
    "    \n",
    "def ccb_actions(file):\n",
    "    return DsJson.ccb_actions(open(file, 'r', encoding='utf-8'))\n",
    "\n",
    "def ccb_slots(file):\n",
    "    return DsJson.ccb_events(open(file, 'r', encoding='utf-8'))\n",
    "\n",
    "def dangling_rewards(file):\n",
    "    return DsJson.dangling_rewards(open(file, 'r', encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame([{'i': i} for i in range(16)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(frac = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
